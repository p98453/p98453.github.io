<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>pytorch基础操作 | 小牛壮士</title><meta name="author" content="kukudelin"><meta name="copyright" content="kukudelin"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="前言：​    此篇主要涉及pytorch的入门内容，主要是张量的基础操作以及使用pytorch进行反向传播，最后还简要介绍了pytorch的一系列常用的基础组件，以便对后续更深层次的学习任务奠定基础。 一、创建张量1.1 简单张量根据已有数据创建张量,torch.tensor默认数字类型是float32 123456789101112131415def test0():    # 创建标量">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch基础操作">
<meta property="og:url" content="http://example.com/2025/05/20/pytorch%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C/index.html">
<meta property="og:site_name" content="小牛壮士">
<meta property="og:description" content="前言：​    此篇主要涉及pytorch的入门内容，主要是张量的基础操作以及使用pytorch进行反向传播，最后还简要介绍了pytorch的一系列常用的基础组件，以便对后续更深层次的学习任务奠定基础。 一、创建张量1.1 简单张量根据已有数据创建张量,torch.tensor默认数字类型是float32 123456789101112131415def test0():    # 创建标量">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/%E6%96%87%E7%AB%A0%E9%BB%98%E8%AE%A4%E5%B0%81%E9%9D%A2.png">
<meta property="article:published_time" content="2025-05-20T07:20:23.000Z">
<meta property="article:modified_time" content="2025-08-12T05:44:11.247Z">
<meta property="article:author" content="kukudelin">
<meta property="article:tag" content="Python基础 OpenCV NLP 大模型">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/%E6%96%87%E7%AB%A0%E9%BB%98%E8%AE%A4%E5%B0%81%E9%9D%A2.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "pytorch基础操作",
  "url": "http://example.com/2025/05/20/pytorch%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C/",
  "image": "http://example.com/img/%E6%96%87%E7%AB%A0%E9%BB%98%E8%AE%A4%E5%B0%81%E9%9D%A2.png",
  "datePublished": "2025-05-20T07:20:23.000Z",
  "dateModified": "2025-08-12T05:44:11.247Z",
  "author": [
    {
      "@type": "Person",
      "name": "kukudelin",
      "url": "http://example.com"
    }
  ]
}</script><link rel="shortcut icon" href="/img/%E5%A4%B4%E5%83%8F.png"><link rel="canonical" href="http://example.com/2025/05/20/pytorch%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":300,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'pytorch基础操作',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/fonts/fonts.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background-image: url(/img/%E4%B8%BB%E9%A1%B5%E8%83%8C%E6%99%AF%E5%9B%BE.png);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/%E5%A4%B4%E5%83%8F.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">35</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/categories"><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/%E6%96%87%E7%AB%A0%E9%BB%98%E8%AE%A4%E5%B0%81%E9%9D%A2.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">小牛壮士</span></a><a class="nav-page-title" href="/"><span class="site-name">pytorch基础操作</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/categories"><span> 分类</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">pytorch基础操作</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-05-20T07:20:23.000Z" title="发表于 2025-05-20 15:20:23">2025-05-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-08-12T05:44:11.247Z" title="更新于 2025-08-12 13:44:11">2025-08-12</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9F%BA%E7%A1%80/">人工智能基础</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">5.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>28分钟</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="前言："><a href="#前言：" class="headerlink" title="前言："></a>前言：</h1><p>​    <strong>此篇主要涉及pytorch的入门内容，主要是张量的基础操作以及使用pytorch进行反向传播，最后还简要介绍了pytorch的一系列常用的基础组件，以便对后续更深层次的学习任务奠定基础。</strong></p>
<h1 id="一、创建张量"><a href="#一、创建张量" class="headerlink" title="一、创建张量"></a>一、创建张量</h1><h2 id="1-1-简单张量"><a href="#1-1-简单张量" class="headerlink" title="1.1 简单张量"></a>1.1 简单张量</h2><p><strong>根据已有数据创建张量,torch.tensor默认数字类型是float32</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test0</span>():</span><br><span class="line">    <span class="comment"># 创建标量</span></span><br><span class="line">    data = torch.tensor(<span class="number">10</span>)</span><br><span class="line">    <span class="built_in">print</span>(data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用numpy数组创建张量</span></span><br><span class="line">    data = np.random.randn(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">    data = torch.tensor(data)</span><br><span class="line">    <span class="built_in">print</span>(data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用列表创建多维张量</span></span><br><span class="line">    data = [[[i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)],[j <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>,<span class="number">20</span>)]]]</span><br><span class="line">    data = torch.tensor(data)</span><br><span class="line">    <span class="built_in">print</span>(data)</span><br><span class="line">test0()</span><br></pre></td></tr></table></figure>



<p>tensor(10)<br> tensor([[ 0.5558,  0.0213,  0.4975],<br>     [ 0.1977,  2.0550, -0.3598]], dtype&#x3D;torch.float64)<br> tensor([[[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],<br>      [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]])</p>
<p><strong>创建指定形状张量</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test1</span>():</span><br><span class="line">    <span class="comment"># 创建两行三列张量</span></span><br><span class="line">    data = torch.Tensor(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">    <span class="built_in">print</span>(data)</span><br><span class="line">    <span class="comment"># 创建指定值张量</span></span><br><span class="line">    data = torch.Tensor([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">    <span class="built_in">print</span>(data)</span><br><span class="line"></span><br><span class="line">test1()</span><br></pre></td></tr></table></figure>



<p>tensor([[1., 1., 1.],<br>     [1., 1., 1.]])<br> tensor([2., 3.])</p>
<p><strong>创建指定类型张量,传递数据如果不匹配会发生类型转换</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test2</span>():</span><br><span class="line">    <span class="comment"># 创建int类型张量</span></span><br><span class="line">    data = torch.IntTensor(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">    <span class="built_in">print</span>(data)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 创建float类型张量</span></span><br><span class="line">    data = torch.FloatTensor(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">    <span class="built_in">print</span>(data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">test2()</span><br></pre></td></tr></table></figure>



<p>tensor([[772014944,    1301,     0],<br>     [     0,     0,     0]], dtype&#x3D;torch.int32)<br> tensor([[0., 0., 0.],<br>     [0., 0., 0.]])</p>
<h2 id="1-2-线性张量和随机张量"><a href="#1-2-线性张量和随机张量" class="headerlink" title="1.2 线性张量和随机张量"></a>1.2 线性张量和随机张量</h2><p><strong>创建线性张量</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test3</span>():</span><br><span class="line">    <span class="comment"># 创建指定步长张量</span></span><br><span class="line">    <span class="comment"># arange(start,end,步长)</span></span><br><span class="line">    data = torch.arange(<span class="number">0</span>,<span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">    data = torch.Tensor(data)</span><br><span class="line">    <span class="built_in">print</span>(data)</span><br><span class="line">    <span class="comment"># 指定区间指定元素个数</span></span><br><span class="line">    <span class="comment"># linspace(start,end,元素个数)</span></span><br><span class="line">    data = torch.linspace(<span class="number">0</span>,<span class="number">10</span>,<span class="number">100</span>)</span><br><span class="line">    data = torch.Tensor(data)</span><br><span class="line">    <span class="built_in">print</span>(data)</span><br><span class="line">test3()</span><br></pre></td></tr></table></figure>



<p>tensor([0, 2, 4, 6, 8])<br> tensor([ 0.0000,  0.1010,  0.2020,  0.3030,  0.4040,  0.5051,  0.6061,  0.7071,<br>      0.8081,  0.9091,  1.0101,  1.1111,  1.2121,  1.3131,  1.4141,  1.5152,<br>      1.6162,  1.7172,  1.8182,  1.9192,  2.0202,  2.1212,  2.2222,  2.3232,<br>      2.4242,  2.5253,  2.6263,  2.7273,  2.8283,  2.9293,  3.0303,  3.1313,<br>      3.2323,  3.3333,  3.4343,  3.5354,  3.6364,  3.7374,  3.8384,  3.9394,<br>      4.0404,  4.1414,  4.2424,  4.3434,  4.4444,  4.5455,  4.6465,  4.7475,<br>      4.8485,  4.9495,  5.0505,  5.1515,  5.2525,  5.3535,  5.4545,  5.5556,<br>      5.6566,  5.7576,  5.8586,  5.9596,  6.0606,  6.1616,  6.2626,  6.3636,<br>      6.4646,  6.5657,  6.6667,  6.7677,  6.8687,  6.9697,  7.0707,  7.1717,<br>      7.2727,  7.3737,  7.4747,  7.5758,  7.6768,  7.7778,  7.8788,  7.9798,<br>      8.0808,  8.1818,  8.2828,  8.3838,  8.4848,  8.5859,  8.6869,  8.7879,<br>      8.8889,  8.9899,  9.0909,  9.1919,  9.2929,  9.3939,  9.4949,  9.5960,<br>      9.6970,  9.7980,  9.8990, 10.0000])</p>
<p><strong>创建随机张量</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test4</span>():</span><br><span class="line">    <span class="comment"># 随机种子</span></span><br><span class="line">    torch.random.manual_seed(<span class="number">42</span>)</span><br><span class="line">    data = torch.randn(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">    data = torch.Tensor(data)</span><br><span class="line">    <span class="built_in">print</span>(data)</span><br><span class="line">test4()</span><br></pre></td></tr></table></figure>



<p>tensor([[ 0.3367,  0.1288,  0.2345],<br>     [ 0.2303, -1.1229, -0.1863]])</p>
<h2 id="1-3-指定值张量"><a href="#1-3-指定值张量" class="headerlink" title="1.3 指定值张量"></a>1.3 指定值张量</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test5</span>():</span><br><span class="line">    <span class="comment"># 创建全为零的张量</span></span><br><span class="line">    data = torch.zeros(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">    <span class="built_in">print</span>(data)</span><br><span class="line">    <span class="comment"># 根据其他张量的形状创建全0张量</span></span><br><span class="line">    data1 = torch.zeros_like(data)</span><br><span class="line">    <span class="built_in">print</span>(data1)</span><br><span class="line">    <span class="comment"># 创建全为一的张量,ones_like同理</span></span><br><span class="line">    data = torch.ones(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">    <span class="built_in">print</span>(data)</span><br><span class="line">    <span class="comment"># 创建指定值张量,torch.fill(形状，填充值),full_like同理</span></span><br><span class="line">    data = torch.full([<span class="number">2</span>,<span class="number">3</span>],<span class="number">6</span>)</span><br><span class="line">    <span class="built_in">print</span>(data)</span><br><span class="line">test5()</span><br></pre></td></tr></table></figure>



<p>tensor([[0., 0., 0.],<br>     [0., 0., 0.]])<br> tensor([[0., 0., 0.],<br>     [0., 0., 0.]])<br> tensor([[1., 1., 1.],<br>     [1., 1., 1.]])<br> tensor([[6, 6, 6],<br>     [6, 6, 6]])</p>
<h2 id="1-4-张量元素类型转换"><a href="#1-4-张量元素类型转换" class="headerlink" title="1.4 张量元素类型转换"></a>1.4 张量元素类型转换</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test6</span>():</span><br><span class="line">    <span class="comment"># type函数转换</span></span><br><span class="line">    data = torch.full([<span class="number">2</span>,<span class="number">3</span>],<span class="number">6</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;转换前&quot;</span>,data.dtype)</span><br><span class="line">    <span class="comment"># 会返回一个新类型的变量而不是直接修改原张量</span></span><br><span class="line">    data1 = data.<span class="built_in">type</span>(torch.FloatTensor)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;转换后&quot;</span>,data1.dtype)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用具体类型函数转换</span></span><br><span class="line">    data2 = data.<span class="built_in">float</span>()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;转换后&quot;</span>,data2.dtype)</span><br><span class="line">test6()</span><br></pre></td></tr></table></figure>



<p>转换前 torch.int64<br> 转换后 torch.float32<br> 转换后 torch.float32</p>
<h1 id="二、数值计算"><a href="#二、数值计算" class="headerlink" title="二、数值计算"></a>二、数值计算</h1><h2 id="2-1-张量的基本运算"><a href="#2-1-张量的基本运算" class="headerlink" title="2.1 张量的基本运算"></a>2.1 张量的基本运算</h2><p><strong>加减乘除相反数</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test0</span>():</span><br><span class="line">    torch.random.manual_seed(<span class="number">40</span>)</span><br><span class="line">    <span class="comment"># torch.randint(start,end,形状)</span></span><br><span class="line">    data = torch.randint(<span class="number">0</span>,<span class="number">10</span>,[<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">    <span class="built_in">print</span>(data)</span><br><span class="line">    <span class="comment"># 计算完成后会产生一个新的张量,满足计算广播机制</span></span><br><span class="line">    <span class="comment"># 加法</span></span><br><span class="line">    tensor = torch.randint(<span class="number">0</span>,<span class="number">10</span>,[<span class="number">2</span>,<span class="number">1</span>])</span><br><span class="line">    <span class="built_in">print</span>(tensor)</span><br><span class="line">    data_add = data.add(tensor)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;data_add&quot;</span>,data_add)</span><br><span class="line">    <span class="comment"># 减法</span></span><br><span class="line">    data_sub = data.sub(tensor)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;data_sub&quot;</span>,data_sub)</span><br><span class="line">    <span class="comment"># 乘法</span></span><br><span class="line">    data_mul = data.mul(data_sub)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;data_mul&quot;</span>,data_mul)</span><br><span class="line">    <span class="comment"># 除法</span></span><br><span class="line">    data_div = data.div(tensor)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;data_div&quot;</span>,data_div)</span><br><span class="line">    <span class="comment"># 取相反数</span></span><br><span class="line">    data_neg = data.neg()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;data_neg&quot;</span>,data_neg)</span><br><span class="line">test0()</span><br></pre></td></tr></table></figure>



<p>tensor([[8, 3, 5],<br>     [7, 2, 4]])<br> tensor([[6],<br>     [3]])<br> data_add tensor([[14,  9, 11],<br>     [10,  5,  7]])<br> data_sub tensor([[ 2, -3, -1],<br>     [ 4, -1,  1]])<br> data_mul tensor([[16, -9, -5],<br>     [28, -2,  4]])<br> data_div tensor([[1.3333, 0.5000, 0.8333],<br>     [2.3333, 0.6667, 1.3333]])<br> data_neg tensor([[-8, -3, -5],</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test2</span>():</span><br><span class="line">    <span class="comment"># 使用mul函数</span></span><br><span class="line">    data1 = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">    data2 = torch.tensor([[<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>]])</span><br><span class="line">    data = data1.mul(data2)</span><br><span class="line">    <span class="built_in">print</span>(data)</span><br><span class="line">    <span class="comment"># 使用*</span></span><br><span class="line">    data = data1*data2</span><br><span class="line">    <span class="built_in">print</span>(data)</span><br><span class="line">test2()</span><br></pre></td></tr></table></figure>




<pre><code> [-7, -2, -4]])
</code></pre>
<p><strong>修改原数据的运算，不需要额外变量来接收运算结果</strong></p>
<p><strong>直接在原有运算方法名后面加下划线</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test1</span>():</span><br><span class="line">    torch.random.manual_seed(<span class="number">40</span>)</span><br><span class="line">    data = torch.randint(<span class="number">0</span>,<span class="number">10</span>,[<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">    <span class="built_in">print</span>(data)</span><br><span class="line">    tensor = torch.randint(<span class="number">0</span>,<span class="number">10</span>,[<span class="number">2</span>,<span class="number">1</span>])</span><br><span class="line">    <span class="built_in">print</span>(tensor)</span><br><span class="line">    data.add_(tensor)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;result&quot;</span>,data)</span><br><span class="line">test1()</span><br></pre></td></tr></table></figure>



<p>tensor([[8, 3, 5],<br>     [7, 2, 4]])<br> tensor([[6],<br>     [3]])<br> result tensor([[14,  9, 11],<br>     [10,  5,  7]])</p>
<h2 id="2-2-阿达玛积"><a href="#2-2-阿达玛积" class="headerlink" title="2.2 阿达玛积"></a>2.2 阿达玛积</h2><p><strong>阿达玛积:张量对应位置元素相乘</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test2</span>():</span><br><span class="line">    <span class="comment"># 使用mul函数</span></span><br><span class="line">    data1 = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">    data2 = torch.tensor([[<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>]])</span><br><span class="line">    data = data1.mul(data2)</span><br><span class="line">    <span class="built_in">print</span>(data)</span><br><span class="line">    <span class="comment"># 使用*</span></span><br><span class="line">    data = data1*data2</span><br><span class="line">    <span class="built_in">print</span>(data)</span><br><span class="line">test2()</span><br></pre></td></tr></table></figure>



<p>tensor([[ 5, 12],<br>     [21, 32]])<br> tensor([[ 5, 12],<br>     [21, 32]])</p>
<h2 id="2-3-点积运算"><a href="#2-3-点积运算" class="headerlink" title="2.3 点积运算"></a>2.3 点积运算</h2><img src="/2025/05/20/pytorch%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C/a53c7b209ec74b379e2dd4888b75d370.png" class="" title="img">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test3</span>():</span><br><span class="line">    <span class="comment"># 使用@运算符直接对两个二维矩阵进行计算</span></span><br><span class="line">    data1 = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>],[<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">    data2 = torch.tensor([[<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>]])</span><br><span class="line">    data = data1@data2</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;@&quot;</span>,data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用mm函数，要求输入张量形状都是二维的</span></span><br><span class="line">    data1 = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>],[<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">    data2 = torch.tensor([[<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>]])</span><br><span class="line">    data = torch.mm(data1,data2)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;mm&quot;</span>,data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用bmm函数运算，要求输入数据必须是三维</span></span><br><span class="line">    torch.random.manual_seed(<span class="number">40</span>)</span><br><span class="line">    <span class="comment"># torch.randn(批次，行，列)</span></span><br><span class="line">    data1 = torch.randn(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">    <span class="built_in">print</span>(data1)</span><br><span class="line">    data2 = torch.randn(<span class="number">3</span>,<span class="number">5</span>,<span class="number">8</span>)</span><br><span class="line">    data = torch.bmm(data1,data2)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;bmm&quot;</span>,data.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用matmul函数</span></span><br><span class="line">    data1 = torch.randn(<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">    data2 = torch.randn(<span class="number">5</span>,<span class="number">6</span>)</span><br><span class="line">    data = torch.matmul(data1,data2)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;matmul1&quot;</span>,data.shape)</span><br><span class="line"></span><br><span class="line">    data1 = torch.randn(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">    data2 = torch.randn(<span class="number">3</span>,<span class="number">5</span>,<span class="number">8</span>)</span><br><span class="line">    data = torch.matmul(data1,data2)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;matmul2&quot;</span>,data.shape)</span><br><span class="line">test3()</span><br></pre></td></tr></table></figure>



<p>@ tensor([[19, 22],<br>     [43, 50],<br>     [67, 78]])<br> mm tensor([[19, 22],<br>     [43, 50],<br>     [67, 78]])<br> tensor([[[-0.2367,  1.8109,  0.1966, -0.7150,  0.1041],<br>      [ 0.8893, -0.4212, -0.5279,  0.9281,  0.8614],<br>      [-0.5856,  1.4157, -1.9983, -0.7397, -1.1917],<br>      [ 0.0635, -1.3966,  0.4813, -1.2866,  0.1643]],</p>
<p>​    [[ 0.4827,  0.4881, -1.8173,  1.0127,  1.3802],<br>​      [ 0.0903,  0.7811,  0.0891,  0.1531, -0.2344],<br>​      [ 0.0868, -1.5610, -0.5121, -0.9283,  1.0775],<br>​      [ 0.1593, -1.8646,  0.5430,  1.4348,  1.1829]],</p>
<p>​    [[-0.3522,  0.9055, -0.1248, -0.1938, -0.3097],<br>​      [-1.4255,  1.2274, -1.7690, -1.6122,  0.4332],<br>​      [ 1.6599,  1.6113,  0.1054,  0.1470,  1.1172],<br>​      [-0.2970, -0.1661, -0.8077, -0.0500,  0.1276]]])<br> bmm torch.Size([3, 4, 8])<br> matmul1 torch.Size([4, 6])<br> matmul2 torch.Size([3, 4, 8])</p>
<h1 id="三、-数值转换"><a href="#三、-数值转换" class="headerlink" title="三、 数值转换"></a>三、 数值转换</h1><h2 id="3-1-tensor张量转numpy数组"><a href="#3-1-tensor张量转numpy数组" class="headerlink" title="3.1 tensor张量转numpy数组"></a>3.1 tensor张量转numpy数组</h2><p><strong>转换后的两个数据共享内存地址，可以使用copy()来区分开来</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test0</span>():</span><br><span class="line">    data_tensor = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">    <span class="comment"># 将张量转换为numpy数组</span></span><br><span class="line">    data_np = data_tensor.numpy()</span><br><span class="line">    <span class="built_in">print</span>(data_tensor,<span class="built_in">type</span>(data_tensor))</span><br><span class="line">    <span class="built_in">print</span>(data_np,<span class="built_in">type</span>(data_np))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 此时两个数据共享内存</span></span><br><span class="line">    data_tensor[<span class="number">0</span>] = <span class="number">2</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;修改tensor后&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(data_tensor)</span><br><span class="line">    <span class="built_in">print</span>(data_np)</span><br><span class="line">    </span><br><span class="line">    data_np[<span class="number">0</span>] = <span class="number">3</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;修改numpy后&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(data_tensor)</span><br><span class="line">    <span class="built_in">print</span>(data_np)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用copy函数实现不共享内存</span></span><br><span class="line">    data_tensor = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">    data_np = data_tensor.numpy().copy()</span><br><span class="line">    data_tensor[<span class="number">0</span>] = <span class="number">2</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;修改copy的tensor后&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(data_tensor)</span><br><span class="line">    <span class="built_in">print</span>(data_np)</span><br><span class="line"></span><br><span class="line">    data_np[<span class="number">0</span>] = <span class="number">3</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;修改copy的numpy后&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(data_tensor)</span><br><span class="line">    <span class="built_in">print</span>(data_np)</span><br><span class="line"></span><br><span class="line">test0()</span><br></pre></td></tr></table></figure>



<p>tensor([1, 2, 3]) &lt;class ‘torch.Tensor’&gt;<br> [1 2 3] &lt;class ‘numpy.ndarray’&gt;<br> 修改tensor后<br> tensor([2, 2, 3])<br> [2 2 3]<br> 修改numpy后<br> tensor([3, 2, 3])<br> [3 2 3]<br> 修改copy的tensor后<br> tensor([2, 2, 3])<br> [1 2 3]<br> 修改copy的numpy后<br> tensor([2, 2, 3])<br> [3 2 3]</p>
<h2 id="3-2-numpy数组转tensor张量"><a href="#3-2-numpy数组转tensor张量" class="headerlink" title="3.2 numpy数组转tensor张量"></a>3.2 numpy数组转tensor张量</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test1</span>():</span><br><span class="line">  data_numpy = np.array([<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">  <span class="comment"># from_numpy函数,默认共享内存</span></span><br><span class="line">  data_tensor1 = torch.from_numpy(data_numpy)</span><br><span class="line">  <span class="comment"># 设置不共享地址</span></span><br><span class="line">  data_tensor_copy = torch.from_numpy(data_numpy.copy())</span><br><span class="line">  <span class="built_in">print</span>(data_tensor1)</span><br><span class="line">  <span class="comment"># 使用tensor，默认不共享内存</span></span><br><span class="line">  data_tensor2 = torch.tensor(data_numpy)</span><br><span class="line">  <span class="built_in">print</span>(data_tensor2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">test1()</span><br></pre></td></tr></table></figure>



<p>tensor([2, 3, 4])<br> tensor([2, 3, 4])</p>
<h2 id="3-3-标量张量和数字的转换"><a href="#3-3-标量张量和数字的转换" class="headerlink" title="3.3 标量张量和数字的转换"></a>3.3 标量张量和数字的转换</h2><p><strong>使用item()提取数字，只适用于张量中只有一个元素的情况</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test2</span>():</span><br><span class="line">    data1 = torch.tensor(<span class="number">0</span>)</span><br><span class="line">    data2 = torch.tensor([<span class="number">10</span>])</span><br><span class="line">    data3 = torch.tensor([[<span class="number">20</span>]])</span><br><span class="line">    <span class="built_in">print</span>(data1.shape,data2.shape,data3.shape)</span><br><span class="line"></span><br><span class="line">    num1 = data1.item()</span><br><span class="line">    num2 = data2.item()</span><br><span class="line">    num3 = data3.item()</span><br><span class="line">    <span class="built_in">print</span>(num1,num2,num3)</span><br><span class="line">test2()</span><br></pre></td></tr></table></figure>



<p>torch.Size([]) torch.Size([1]) torch.Size([1, 1])<br> 0 10 20</p>
<h1 id="四、拼接操作"><a href="#四、拼接操作" class="headerlink" title="四、拼接操作"></a>四、拼接操作</h1><h2 id="4-1-torch-cat（连接操作）"><a href="#4-1-torch-cat（连接操作）" class="headerlink" title="4.1 torch.cat（连接操作）"></a><strong>4.1 torch.cat（连接操作）</strong></h2><ul>
<li>功能：沿指定维度连接多个张量，输入张量的形状（除拼接维度外）必须完全相同。</li>
<li>不新增维度，仅扩展现有维度的大小。</li>
<li>适用于需要合并数据但无需新增维度的场景（如拼接多个特征向量）。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">y = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 沿维度0拼接（行方向）</span></span><br><span class="line">z_cat = torch.cat([x, y], dim=<span class="number">0</span>)  <span class="comment"># 输出形状：(4, 3)</span></span><br></pre></td></tr></table></figure>



<h2 id="4-2-torch-stack（堆叠操作）"><a href="#4-2-torch-stack（堆叠操作）" class="headerlink" title="4.2 torch.stack（堆叠操作）"></a><strong>4.2 torch.stack（堆叠操作）</strong></h2><ul>
<li>功能：在新创建的维度上堆叠多个张量，所有输入张量的形状必须完全一致。</li>
<li>新增一个维度，堆叠后的张量比输入张量多一维。</li>
<li>适用于需要创建批次维度或组合多个张量的场景（如创建图像批次）。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 相同输入张量x和y</span></span><br><span class="line">z_stack = torch.stack([x, y], dim=<span class="number">0</span>)  <span class="comment"># 输出形状：(2, 2, 3)</span></span><br></pre></td></tr></table></figure>



<p><strong>关键差异总结</strong></p>
<ul>
<li><strong>维度变化</strong>：<code>cat</code>不新增维度，<code>stack</code>会新增一个维度。</li>
<li><strong>输入要求</strong>：<code>cat</code>允许拼接维度的大小不同（其他维度相同），<code>stack</code>要求所有维度完全一致。</li>
<li><strong>典型用途</strong>：<code>cat</code>用于合并数据序列，<code>stack</code>用于构建批次或组合张量。</li>
</ul>
<h1 id="五、索引操作"><a href="#五、索引操作" class="headerlink" title="五、索引操作"></a>五、索引操作</h1><h2 id="5-1-简单行列索引"><a href="#5-1-简单行列索引" class="headerlink" title="5.1 简单行列索引"></a>5.1 简单行列索引</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test0</span>():</span><br><span class="line">    torch.random.manual_seed(<span class="number">42</span>)</span><br><span class="line">    data = torch.randint(<span class="number">0</span>,<span class="number">10</span>,(<span class="number">4</span>,<span class="number">5</span>))</span><br><span class="line">    <span class="built_in">print</span>(data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获得某行某列元素</span></span><br><span class="line">    <span class="built_in">print</span>(data[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(data[<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">    <span class="built_in">print</span>(data[<span class="number">0</span>][<span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 左行右列，左闭右开</span></span><br><span class="line">    <span class="built_in">print</span>(data[:<span class="number">3</span>,<span class="number">2</span>:<span class="number">3</span>])</span><br><span class="line">    <span class="built_in">print</span>(data[:<span class="number">3</span>][<span class="number">2</span>:<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">test0()</span><br></pre></td></tr></table></figure>



<p> tensor([[2, 7, 6, 4, 6],<br>     [5, 0, 4, 0, 3],<br>     [8, 4, 0, 4, 1],<br>     [2, 5, 5, 7, 6]])<br> tensor([2, 7, 6, 4, 6])<br> tensor(7)<br> tensor(7)<br> tensor([[6],<br>     [4],<br>     [0]])<br> tensor(7)</p>
<h2 id="5-2-布尔索引"><a href="#5-2-布尔索引" class="headerlink" title="5.2 布尔索引"></a>5.2 布尔索引</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test1</span>():</span><br><span class="line">    torch.random.manual_seed(<span class="number">42</span>)</span><br><span class="line">    data = torch.randint(<span class="number">0</span>,<span class="number">100</span>,(<span class="number">4</span>,<span class="number">5</span>))</span><br><span class="line">    <span class="built_in">print</span>(data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取所有大于30的元素</span></span><br><span class="line">    <span class="built_in">print</span>(data[data&gt;<span class="number">30</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取第二列有元素大于20的行</span></span><br><span class="line">    <span class="built_in">print</span>(data[data[:,<span class="number">1</span>]&gt;=<span class="number">20</span>])</span><br><span class="line"></span><br><span class="line">test1()</span><br></pre></td></tr></table></figure>



<p>tensor([[42, 67, 76, 14, 26],<br>     [35, 20, 24, 50, 13],<br>     [78, 14, 10, 54, 31],<br>     [72, 15, 95, 67,  6]])<br> tensor([42, 67, 76, 35, 50, 78, 54, 31, 72, 95, 67])<br> tensor([[42, 67, 76, 14, 26],<br>     [35, 20, 24, 50, 13]])</p>
<h2 id="5-3-多维索引"><a href="#5-3-多维索引" class="headerlink" title="5.3 多维索引"></a>5.3 多维索引</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test2</span>():</span><br><span class="line">    torch.random.manual_seed(<span class="number">42</span>)</span><br><span class="line">    data = torch.randint(<span class="number">0</span>,<span class="number">100</span>,(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>))</span><br><span class="line">    <span class="built_in">print</span>(data)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(data[<span class="number">0</span>,:,:])</span><br><span class="line">    <span class="built_in">print</span>(data[:,<span class="number">0</span>,:])</span><br><span class="line">    <span class="built_in">print</span>(data[:,:,<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">test2()</span><br></pre></td></tr></table></figure>



<p>tensor([[[42, 67, 76, 14, 26],<br>      [35, 20, 24, 50, 13],<br>      [78, 14, 10, 54, 31],<br>      [72, 15, 95, 67,  6]],</p>
<p>​    [[49, 76, 73, 11, 99],<br>​      [13, 41, 69, 87, 19],<br>​      [72, 80, 75, 29, 33],<br>​      [64, 39, 76, 32, 10]],</p>
<p>​    [[86, 22, 77, 19,  7],<br>​      [23, 43, 94, 93, 77],<br>​      [70,  9, 70, 39, 86],<br>​      [99, 15, 84, 78,  8]]])<br> tensor([[42, 67, 76, 14, 26],<br>​     [35, 20, 24, 50, 13],<br>​     [78, 14, 10, 54, 31],<br>​     [72, 15, 95, 67,  6]])<br> tensor([[42, 67, 76, 14, 26],<br>​     [49, 76, 73, 11, 99],<br>​     [86, 22, 77, 19,  7]])<br> tensor([[42, 35, 78, 72],<br>​     [49, 13, 72, 64],<br>​     [86, 23, 70, 99]])</p>
<h1 id="六、形状操作"><a href="#六、形状操作" class="headerlink" title="六、形状操作"></a>六、形状操作</h1><img src="/2025/05/20/pytorch%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C/5a47b7c5520648a0b4c46750fc7f0b9b.png" class="" title="img">

<h2 id="6-1-reshape-函数"><a href="#6-1-reshape-函数" class="headerlink" title="6.1 reshape()函数"></a>6.1 reshape()函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test0</span>():</span><br><span class="line">  torch.random.manual_seed(<span class="number">42</span>)</span><br><span class="line">  data = torch.randint(<span class="number">0</span>,<span class="number">10</span>,[<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">  <span class="built_in">print</span>(data.shape)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 修改张量形状</span></span><br><span class="line">  new_data = torch.reshape(data,[<span class="number">2</span>,<span class="number">10</span>])</span><br><span class="line">  <span class="built_in">print</span>(new_data.shape)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 使用-1自动匹配形状</span></span><br><span class="line">  new_data = torch.reshape(data,[-<span class="number">1</span>,<span class="number">20</span>])</span><br><span class="line">  <span class="built_in">print</span>(new_data.shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">test0()</span><br></pre></td></tr></table></figure>



<p>torch.Size([4, 5])<br> torch.Size([2, 10])<br> torch.Size([1, 20])</p>
<h2 id="6-2-transpose-函数和permute-函数"><a href="#6-2-transpose-函数和permute-函数" class="headerlink" title="6.2 transpose()函数和permute()函数"></a>6.2 transpose()函数和permute()函数</h2><p><strong>transpose只是简单进行维度交换,一次只能交换两个维度,permute函数在transpose基础上可以一次交换多个维度</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test1</span>():</span><br><span class="line">    torch.random.manual_seed(<span class="number">42</span>)</span><br><span class="line">    data = torch.randint(<span class="number">0</span>,<span class="number">10</span>,[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">    <span class="built_in">print</span>(data.shape)</span><br><span class="line">    <span class="built_in">print</span>(data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-&quot;</span>*<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># reshape重新计算形状</span></span><br><span class="line">    data_reshape = torch.reshape(data,[<span class="number">4</span>,<span class="number">3</span>,<span class="number">5</span>])</span><br><span class="line">    <span class="built_in">print</span>(data_reshape.shape)</span><br><span class="line">    <span class="built_in">print</span>(data_reshape)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-&quot;</span>*<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line">    data_transpose = torch.transpose(data,<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(data_transpose.shape)</span><br><span class="line">    <span class="built_in">print</span>(data_transpose)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-&quot;</span>*<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line">    data_permute = torch.permute(data,(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line">    <span class="built_in">print</span>(data_permute.shape)</span><br><span class="line">    <span class="built_in">print</span>(data_permute)</span><br><span class="line"></span><br><span class="line">test1()</span><br></pre></td></tr></table></figure>



<h2 id="6-3-view-函数和contigous-函数"><a href="#6-3-view-函数和contigous-函数" class="headerlink" title="6.3 view()函数和contigous()函数"></a>6.3 view()函数和contigous()函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test1</span>():</span><br><span class="line">    <span class="comment"># view函数的使用</span></span><br><span class="line">    data = torch.tensor([[<span class="number">10</span>,<span class="number">20</span>,<span class="number">30</span>],[<span class="number">40</span>,<span class="number">50</span>,<span class="number">60</span>]])</span><br><span class="line">    data_view = data.view(<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">    <span class="built_in">print</span>(data_view.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用is_contigous判断张量是否是连续的内存空间</span></span><br><span class="line">    <span class="built_in">print</span>(data.is_contiguous())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果不是连续的内存空间，不可以使用view来更改维度</span></span><br><span class="line">    data_transpose = torch.transpose(data,<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(data_transpose.is_contiguous())<span class="comment"># 非连续</span></span><br><span class="line">    <span class="comment"># print(data_transpose.view(2,3))# 非连续地址，报错</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用contiguous()将非连续内存改为连续内存</span></span><br><span class="line">    <span class="built_in">print</span>(data_transpose.contiguous().view(<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">test1()</span><br></pre></td></tr></table></figure>



<p>torch.Size([3, 2])<br> True<br> False<br> tensor([[10, 40, 20],<br>     [50, 30, 60]])</p>
<h2 id="6-4-squeeze-函数和unsqueeze-函数"><a href="#6-4-squeeze-函数和unsqueeze-函数" class="headerlink" title="6.4 squeeze()函数和unsqueeze()函数"></a>6.4 squeeze()函数和unsqueeze()函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test2</span>():</span><br><span class="line">    <span class="comment"># squeeze函数降维，去除数据中的1维度</span></span><br><span class="line">    data = torch.randint(<span class="number">1</span>,<span class="number">10</span>,[<span class="number">1</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">5</span>])</span><br><span class="line">    <span class="built_in">print</span>(data.shape)</span><br><span class="line">    <span class="built_in">print</span>(data)</span><br><span class="line">    data_squ = data.squeeze()</span><br><span class="line">    <span class="built_in">print</span>(data_squ.shape)</span><br><span class="line">    <span class="built_in">print</span>(data_squ)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 制定去除第二个1的维度</span></span><br><span class="line">    data_squ_1 = data.squeeze(<span class="number">2</span>)</span><br><span class="line">    <span class="built_in">print</span>(data_squ_1.shape)</span><br><span class="line">    <span class="built_in">print</span>(data_squ_1)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-&quot;</span>*<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># unsqueeze函数升维</span></span><br><span class="line">    data = torch.randint(<span class="number">1</span>,<span class="number">10</span>,[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">    <span class="built_in">print</span>(data.shape)</span><br><span class="line">    <span class="comment"># 参数表示升维的位置</span></span><br><span class="line">    data_unsqu = data.unsqueeze(-<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(data_unsqu.shape)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">test2()</span><br></pre></td></tr></table></figure>



<p>torch.Size([1, 3, 1, 5])<br> tensor([[[[3, 4, 3, 8, 1]],</p>
<p>​     [[6, 7, 2, 8, 8]],</p>
<p>​     [[9, 1, 8, 8, 8]]]])<br> torch.Size([3, 5])<br> tensor([[3, 4, 3, 8, 1],<br>​     [6, 7, 2, 8, 8],<br>​     [9, 1, 8, 8, 8]])<br> torch.Size([1, 3, 5])<br> tensor([[[3, 4, 3, 8, 1],<br>​      [6, 7, 2, 8, 8],<br>​      [9, 1, 8, 8, 8]]])<br> -————————————————-<br> torch.Size([3, 4, 5])<br> torch.Size([3, 4, 5, 1])</p>
<h1 id="七、运算函数"><a href="#七、运算函数" class="headerlink" title="七、运算函数"></a>七、运算函数</h1><p><strong>运算规则和numpy类似</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test0</span>():</span><br><span class="line">    torch.random.manual_seed(<span class="number">42</span>)</span><br><span class="line">    data1 = torch.randint(<span class="number">0</span>,<span class="number">5</span>,[<span class="number">2</span>,<span class="number">3</span>],dtype=torch.float64)</span><br><span class="line">    <span class="built_in">print</span>(data1)</span><br><span class="line">    data2 = torch.randint(<span class="number">5</span>,<span class="number">10</span>,[<span class="number">2</span>,<span class="number">3</span>],dtype=torch.float64)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 均值,默认对所有数据计算均值</span></span><br><span class="line">    <span class="built_in">print</span>(data1.mean())</span><br><span class="line">    <span class="comment"># 按照指定维度</span></span><br><span class="line">    <span class="built_in">print</span>(data1.mean(dim=<span class="number">1</span>))<span class="comment"># 行</span></span><br><span class="line">    <span class="built_in">print</span>(data1.mean(dim=<span class="number">0</span>))<span class="comment"># 列</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 求和sum()</span></span><br><span class="line">    <span class="comment"># 平方pow()</span></span><br><span class="line">    <span class="comment"># 平方根sqrt()</span></span><br><span class="line">    <span class="comment"># 指数幂exp()</span></span><br><span class="line">    <span class="comment"># 对数log2()</span></span><br><span class="line"></span><br><span class="line">test0()</span><br></pre></td></tr></table></figure>



<p>tensor([[2., 2., 1.],<br>     [4., 1., 0.]], dtype&#x3D;torch.float64)<br> tensor(1.6667, dtype&#x3D;torch.float64)<br> tensor([1.6667, 1.6667], dtype&#x3D;torch.float64)<br> tensor([3.0000, 1.5000, 0.5000], dtype&#x3D;torch.float64)</p>
<h1 id="八、反向传播"><a href="#八、反向传播" class="headerlink" title="八、反向传播"></a>八、反向传播</h1><h2 id="8-1-梯度基本计算"><a href="#8-1-梯度基本计算" class="headerlink" title="8.1 梯度基本计算"></a>8.1 梯度基本计算</h2><h3 id="8-1-1-标量梯度计算"><a href="#8-1-1-标量梯度计算" class="headerlink" title="8.1.1 标量梯度计算"></a>8.1.1 标量梯度计算</h3><p><strong>对于需要求导的张量需要设置参数为True,并且类型指定为小数</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test0</span>():</span><br><span class="line">    x = torch.tensor(<span class="number">10</span>,requires_grad=<span class="literal">True</span>,dtype=torch.float64)</span><br><span class="line">    f = x**<span class="number">2</span>+<span class="number">20</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># backward进行自动微分,2*x</span></span><br><span class="line">    f.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># grad访问梯度</span></span><br><span class="line">    <span class="built_in">print</span>(x.grad)</span><br><span class="line"></span><br><span class="line">test0()</span><br></pre></td></tr></table></figure>



<p>tensor(20., dtype&#x3D;torch.float64)</p>
<h3 id="8-1-2-向量梯度计算"><a href="#8-1-2-向量梯度计算" class="headerlink" title="8.1.2 向量梯度计算"></a>8.1.2 向量梯度计算</h3><p><strong>上面f计算出来的是一个向量，不能直接求梯度，梯度必须是在标量基础上的</strong></p>
<p><strong>用均值或求和来表示f使f变成标量</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test1</span>():</span><br><span class="line">    x = torch.tensor([<span class="number">10</span>,<span class="number">20</span>,<span class="number">30</span>,<span class="number">40</span>],requires_grad=<span class="literal">True</span>,dtype=torch.float64)</span><br><span class="line"></span><br><span class="line">    f = x**<span class="number">2</span>+<span class="number">20</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;f&quot;</span>,f)</span><br><span class="line"></span><br><span class="line">    f_mean = f.mean() <span class="comment"># f_mean = f/4</span></span><br><span class="line"></span><br><span class="line">    f_mean.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># grad访问梯度</span></span><br><span class="line">    <span class="built_in">print</span>(x.grad)</span><br><span class="line"></span><br><span class="line">test1()</span><br></pre></td></tr></table></figure>



<p>f tensor([ 120.,  420.,  920., 1620.], dtype&#x3D;torch.float64,grad_fn&#x3D;<AddBackward0>)<br> tensor([ 5., 10., 15., 20.], dtype&#x3D;torch.float64)</p>
<h3 id="8-1-3-多标量梯度计算"><a href="#8-1-3-多标量梯度计算" class="headerlink" title="8.1.3 多标量梯度计算"></a>8.1.3 多标量梯度计算</h3><p>多标量其实大差不差，只是最后要分开访问变量的梯度值就行了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test2</span>():</span><br><span class="line">    x1 = torch.tensor(<span class="number">10</span>,requires_grad=<span class="literal">True</span>,dtype=torch.float64)</span><br><span class="line">    x2 = torch.tensor(<span class="number">20</span>,requires_grad=<span class="literal">True</span>,dtype=torch.float64)</span><br><span class="line">    f = x1**<span class="number">2</span>+x2**<span class="number">2</span>+<span class="number">2</span>*x1</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;f&quot;</span>,f)</span><br><span class="line"></span><br><span class="line">    f.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># grad分别访问x1,x2梯度</span></span><br><span class="line">    <span class="built_in">print</span>(x1.grad)</span><br><span class="line">    <span class="built_in">print</span>(x2.grad)</span><br><span class="line"></span><br><span class="line">test2()</span><br></pre></td></tr></table></figure>



<p>f tensor(520., dtype&#x3D;torch.float64, grad_fn&#x3D;<AddBackward0>)<br> tensor(22., dtype&#x3D;torch.float64)<br> tensor(40., dtype&#x3D;torch.float64)</p>
<h3 id="8-1-4-多向量梯度计算"><a href="#8-1-4-多向量梯度计算" class="headerlink" title="8.1.4 多向量梯度计算"></a>8.1.4 多向量梯度计算</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test3</span>():</span><br><span class="line">    x1 = torch.tensor([<span class="number">10</span>,<span class="number">20</span>,<span class="number">30</span>,<span class="number">40</span>],requires_grad=<span class="literal">True</span>,dtype=torch.float64)</span><br><span class="line">    x2 = torch.tensor([<span class="number">40</span>,<span class="number">50</span>,<span class="number">60</span>,<span class="number">70</span>],requires_grad=<span class="literal">True</span>,dtype=torch.float64)</span><br><span class="line">    f = x1**<span class="number">2</span>+x2**<span class="number">2</span>+<span class="number">2</span>*x1</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;f&quot;</span>,f)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 转为标量</span></span><br><span class="line">    f_mean = f.<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">    f_mean.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># grad分别访问x1,x2梯度</span></span><br><span class="line">    <span class="built_in">print</span>(x1.grad)</span><br><span class="line">    <span class="built_in">print</span>(x2.grad)</span><br><span class="line"></span><br><span class="line">test3()</span><br></pre></td></tr></table></figure>



<p>f tensor([1720., 2940., 4560., 6580.], dtype&#x3D;torch.float64,<br>     grad_fn&#x3D;<AddBackward0>)<br> tensor([22., 42., 62., 82.], dtype&#x3D;torch.float64)<br> tensor([ 80., 100., 120., 140.], dtype&#x3D;torch.float64)</p>
<h2 id="8-2-控制梯度计算"><a href="#8-2-控制梯度计算" class="headerlink" title="8.2 控制梯度计算"></a>8.2 控制梯度计算</h2><p>8.2.1 控制梯度计算</p>
<p> <strong>可以通过控制梯度计算，来设定忽略某个变量的梯度，设定后计算梯度将自动跳过</strong></p>
<p><strong>第一种方式：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test4</span>():</span><br><span class="line">    x = torch.tensor(<span class="number">10</span>,requires_grad=<span class="literal">True</span>,dtype=torch.float64)</span><br><span class="line">    <span class="comment"># 此时可以调用backward计算f的梯度</span></span><br><span class="line">    <span class="built_in">print</span>(x.requires_grad)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        f = x**<span class="number">2</span></span><br><span class="line">    <span class="built_in">print</span>(f.requires_grad)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">test4()</span><br></pre></td></tr></table></figure>



<p>True<br> False</p>
<p><strong>第二种方式：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">my_func</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x**<span class="number">2</span></span><br><span class="line">f = my_func(x)</span><br><span class="line"><span class="built_in">print</span>(f.requires_grad)</span><br></pre></td></tr></table></figure>



<p>False </p>
<p><strong>第三种方式：全局禁用梯度</strong></p>
<p>由于 f 是在禁用梯度计算的上下文中生成的，因此会抛出错误</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">torch.set_grad_enabled(<span class="literal">False</span>)</span><br><span class="line">f = x**<span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(f.requires_grad)</span><br><span class="line"></span><br><span class="line">f.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># grad访问梯度</span></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure>



<h3 id="8-2-2-累计梯度和梯度清零"><a href="#8-2-2-累计梯度和梯度清零" class="headerlink" title="8.2.2 累计梯度和梯度清零"></a>8.2.2 累计梯度和梯度清零</h3><p><strong>通过循环重复对x进行计算会将历史梯度值累加到x.grad属性中,相当于通过循环多次求导</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test5</span>():</span><br><span class="line">    x = torch.tensor([<span class="number">10</span>,<span class="number">20</span>,<span class="number">30</span>,<span class="number">40</span>],requires_grad=<span class="literal">True</span>,dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">        f1 = x**<span class="number">2</span>+<span class="number">20</span></span><br><span class="line">        <span class="comment"># 将向量转换为标量</span></span><br><span class="line">        f2 = f1.mean()</span><br><span class="line">        <span class="comment"># 自动微分</span></span><br><span class="line">        f2.backward()</span><br><span class="line">        <span class="built_in">print</span>(x.grad)</span><br><span class="line">test5()  </span><br></pre></td></tr></table></figure>



<p>tensor([ 5., 10., 15., 20.])<br> tensor([10., 20., 30., 40.])<br> tensor([15., 30., 45., 60.]) </p>
<p><strong>但是在实际的梯度下降求解最优参数中，我们不希望这样的累加行为来影响梯度求解的结果，而是每次循环有不一样的数据来对损失函数进行优化，因此每次循环要进行梯度清零</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test6</span>():</span><br><span class="line">    x = torch.tensor([<span class="number">10</span>,<span class="number">20</span>,<span class="number">30</span>,<span class="number">40</span>],requires_grad=<span class="literal">True</span>,dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">        f1 = x**<span class="number">2</span>+<span class="number">20</span></span><br><span class="line">        <span class="comment"># 将向量转换为标量</span></span><br><span class="line">        f2 = f1.mean()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 梯度清零</span></span><br><span class="line">        <span class="keyword">if</span> x.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x.grad.data.zero_()</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># 自动微分</span></span><br><span class="line">        f2.backward()</span><br><span class="line">        <span class="built_in">print</span>(x.grad)</span><br><span class="line">test6() </span><br></pre></td></tr></table></figure>



<p>在PyTorch中，梯度是累加的。这意味着每次调用 <code>.backward()</code> 时，计算得到的梯度会被累加到现有的梯度上，而不是替换它。如果不进行梯度清零，历史梯度会不断累积，导致梯度值变得非常大，这会影响模型的训练效果。 </p>
<h3 id="8-2-3-梯度下降优化算法案例"><a href="#8-2-3-梯度下降优化算法案例" class="headerlink" title="8.2.3 梯度下降优化算法案例"></a>8.2.3 梯度下降优化算法案例</h3><p>在梯度清零的基础上，我们需要更新每次循环后的参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test7</span>():</span><br><span class="line">    <span class="comment"># 初始化一个x和theta</span></span><br><span class="line">    x = torch.tensor(<span class="number">10</span>,requires_grad=<span class="literal">True</span>,dtype=torch.float64)</span><br><span class="line">    theta = <span class="number">0.01</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        y = x**<span class="number">3</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 梯度清零</span></span><br><span class="line">        <span class="keyword">if</span> x.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x.grad.data.zero_()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 自动微分</span></span><br><span class="line">        y.backward()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        x.data = x.data-theta*x.grad</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 打印x值</span></span><br><span class="line">        <span class="built_in">print</span>(x.data)</span><br><span class="line"></span><br><span class="line">test7()</span><br></pre></td></tr></table></figure>



<img src="/2025/05/20/pytorch%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C/0b3e949691234057aaa5e8d03f78db24.png" class="" title="img">

<h2 id="8-3-梯度计算注意"><a href="#8-3-梯度计算注意" class="headerlink" title="8.3 梯度计算注意"></a>8.3 梯度计算注意</h2><img src="/2025/05/20/pytorch%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C/d2c969b0caac4ce89877e360f724a9d3.png" class="" title="img">

<p><code>detach()</code> 方法用于将张量从计算图中分离出来，返回一个新的张量，该张量与原始张量共享数据，但不会参与梯度计算。它的主要用途包括防止梯度传播、节省内存和优化模型推理。 </p>
<h3 id="8-3-1-简单演示"><a href="#8-3-1-简单演示" class="headerlink" title="8.3.1 简单演示"></a>8.3.1 简单演示</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test8</span>():</span><br><span class="line">    x = torch.tensor([<span class="number">10</span>,<span class="number">20</span>],requires_grad=<span class="literal">True</span>,dtype=torch.float64)</span><br><span class="line">    <span class="comment"># 演示上述错误</span></span><br><span class="line">    <span class="comment"># print(x.numpy())</span></span><br><span class="line">    <span class="comment"># 演示正确操作</span></span><br><span class="line">    <span class="built_in">print</span>(x.detach().numpy())</span><br><span class="line">test8()</span><br></pre></td></tr></table></figure>



<h3 id="8-3-2-解决方案：detach-数据共享"><a href="#8-3-2-解决方案：detach-数据共享" class="headerlink" title="8.3.2 解决方案：detach()数据共享"></a>8.3.2 解决方案：detach()数据共享</h3><p>  <strong>解释：</strong></p>
<img src="/2025/05/20/pytorch%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C/c3b54c2819ce4c0a9929b7045396aeb4.png" class="" title="img">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test9</span>():</span><br><span class="line">    <span class="comment"># 演示正确操作</span></span><br><span class="line">    x1 = torch.tensor([<span class="number">10</span>,<span class="number">20</span>],requires_grad=<span class="literal">True</span>,dtype=torch.float64)</span><br><span class="line">    x2 = x1.detach()</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">id</span>(x1.data),<span class="built_in">id</span>(x2.data))</span><br><span class="line">    </span><br><span class="line">    x2[<span class="number">0</span>] = <span class="number">100</span></span><br><span class="line">    <span class="built_in">print</span>(x1)</span><br><span class="line">    </span><br><span class="line">test9()</span><br></pre></td></tr></table></figure>



<p>1607428373376 1607428373376<br> tensor([100.,  20.], dtype&#x3D;torch.float64, requires_grad&#x3D;True) </p>
<h1 id="九、基础组件"><a href="#九、基础组件" class="headerlink" title="九、基础组件"></a>九、基础组件</h1><img src="/2025/05/20/pytorch%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C/77f805350ce14f26a44385ea91e636fb.png" class="" title="img">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_regression</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset,DataLoader,TensorDataset</span><br></pre></td></tr></table></figure>



<h2 id="9-1-基础组件的用法"><a href="#9-1-基础组件的用法" class="headerlink" title="9.1 基础组件的用法"></a>9.1 基础组件的用法</h2><h3 id="9-1-1-损失函数"><a href="#9-1-1-损失函数" class="headerlink" title="9.1.1 损失函数"></a>9.1.1 损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test0</span>():</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 初始化平方损失函数对象</span></span><br><span class="line">  criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 该对象可以当做函数来使用</span></span><br><span class="line">  torch.random.manual_seed(<span class="number">42</span>)</span><br><span class="line">  y_pred = torch.randn(<span class="number">3</span>,<span class="number">5</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">  y_true = torch.randn(<span class="number">3</span>,<span class="number">5</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 计算损失</span></span><br><span class="line">  loss = criterion(y_pred,y_true)</span><br><span class="line">  <span class="built_in">print</span>(loss)  </span><br><span class="line"></span><br><span class="line">test0()</span><br></pre></td></tr></table></figure>



<p>tensor(1.0192, grad_fn&#x3D;<MseLossBackward0>) </p>
<h3 id="9-1-2-线性假设函数"><a href="#9-1-2-线性假设函数" class="headerlink" title="9.1.2 线性假设函数"></a>9.1.2 线性假设函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test1</span>():</span><br><span class="line">    <span class="comment"># 输入数据特征为10，输出数据特征为5</span></span><br><span class="line">    model = nn.Linear(in_features=<span class="number">10</span>,out_features=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建4行10列输入数据</span></span><br><span class="line">    <span class="built_in">input</span> = torch.randn(<span class="number">4</span>,<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    y_pred = model(<span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(y_pred.shape)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">input</span>)</span><br><span class="line">    <span class="built_in">print</span>(y_pred)</span><br><span class="line"></span><br><span class="line">test1()</span><br></pre></td></tr></table></figure>



<p>torch.Size([4, 5])<br> tensor([[ 0.4771,  0.7262,  0.0912, -0.3891,  0.5279,  1.0311, -0.7048,  1.0131,<br>      0.7642,  1.0950],<br>     [ 0.3399,  0.7200,  0.4114, -0.5733,  0.5069, -0.4752, -1.1299, -0.1360,<br>      1.6354,  0.6547],<br>     [ 0.5760, -0.3609, -0.0606,  0.0733,  0.4976, -0.4257, -1.3371, -0.1933,<br>      0.6526, -1.9006],<br>     [ 0.2286,  0.0249,  0.1947, -1.6535,  0.6814,  1.4611, -0.3098, -1.6022,<br>      1.3529,  1.2888]])<br> tensor([[-0.5221,  0.6345, -0.4408, -1.4161, -0.7894],<br>     [-0.3523,  1.0458, -0.2068, -1.1083, -0.2492],<br>     [ 0.3888, -0.0686, -0.2927, -0.1234,  0.3353],<br>     [ 0.2049,  0.3954,  0.5659, -1.4681, -0.4581]],<br>     grad_fn&#x3D;<AddmmBackward0>)</p>
<h3 id="9-1-3-优化器"><a href="#9-1-3-优化器" class="headerlink" title="9.1.3 优化器"></a>9.1.3 优化器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test2</span>():</span><br><span class="line">    model = nn.Linear(in_features=<span class="number">10</span>,out_features=<span class="number">5</span>)</span><br><span class="line">    <span class="comment"># 传入模型参数和学习率</span></span><br><span class="line">    optimizer = optim.SGD(model.parameters(),lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 此处省略backword</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 再调用backword之前要梯度清零</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># 更新模型参数</span></span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>



<h2 id="9-2-数据加载器的使用"><a href="#9-2-数据加载器的使用" class="headerlink" title="9.2 数据加载器的使用"></a>9.2 数据加载器的使用</h2><h3 id="9-2-1-数据类创建"><a href="#9-2-1-数据类创建" class="headerlink" title="9.2.1 数据类创建"></a>9.2.1 数据类创建</h3><p><strong>如果是引用数据集，那么就自带有这个部分内容</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SampleDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,x,y</span>):</span><br><span class="line">        <span class="comment"># 初始化</span></span><br><span class="line">        <span class="variable language_">self</span>.x = x</span><br><span class="line">        <span class="variable language_">self</span>.y = y</span><br><span class="line">        <span class="variable language_">self</span>.<span class="built_in">len</span> = <span class="built_in">len</span>(y)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 返回数据总量</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.<span class="built_in">len</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="comment"># 根据索引返回一条样本</span></span><br><span class="line">        <span class="comment"># 将index限定在合理范围内</span></span><br><span class="line">       index = <span class="built_in">min</span>(<span class="built_in">max</span>(index,<span class="number">0</span>),<span class="variable language_">self</span>.<span class="built_in">len</span>-<span class="number">1</span>)</span><br><span class="line">       <span class="keyword">return</span> <span class="variable language_">self</span>.x[index],<span class="variable language_">self</span>.y[index]</span><br></pre></td></tr></table></figure>



<h3 id="9-2-2-实例化数据加载类"><a href="#9-2-2-实例化数据加载类" class="headerlink" title="9.2.2 实例化数据加载类"></a>9.2.2 实例化数据加载类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test0</span>():</span><br><span class="line">    x = torch.randn(<span class="number">100</span>,<span class="number">8</span>)</span><br><span class="line">    <span class="built_in">print</span>(x.size())</span><br><span class="line">    y = torch.randint(<span class="number">0</span>,<span class="number">2</span>,(x.size(<span class="number">0</span>),))</span><br><span class="line">    <span class="built_in">print</span>(y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    sample_dataset = SampleDataset(x,y)</span><br><span class="line">    <span class="built_in">print</span>(sample_dataset[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line">    <span class="built_in">print</span>(sample_dataset[<span class="number">0</span>][<span class="number">1</span>])</span><br><span class="line">test0()</span><br></pre></td></tr></table></figure>



<p>torch.Size([100, 8])<br> tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,<br>     0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,<br>     0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,<br>     0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,<br>     0, 0, 0, 0])<br> tensor([ 1.1179, -1.2956,  0.0503, -0.5855, -0.3900,  0.0358,  0.1206, -0.8057])<br> tensor(0) </p>
<h3 id="9-2-3-数据加载类的使用"><a href="#9-2-3-数据加载类的使用" class="headerlink" title="9.2.3 数据加载类的使用"></a>9.2.3 数据加载类的使用</h3><p><strong>一次加载多条数据</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test1</span>():</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 构建数据对象</span></span><br><span class="line">    x = torch.randn(<span class="number">100</span>,<span class="number">8</span>)</span><br><span class="line">    y = torch.randint(<span class="number">0</span>,<span class="number">2</span>,(x.size(<span class="number">0</span>),))</span><br><span class="line">    sample_dataset = SampleDataset(x,y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用Dataloader</span></span><br><span class="line">    dataloader = DataLoader(sample_dataset,</span><br><span class="line">                            batch_size=<span class="number">4</span>,</span><br><span class="line">                            shuffle=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> dataloader:</span><br><span class="line">        <span class="built_in">print</span>(x)</span><br><span class="line">        <span class="built_in">print</span>(y)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line">test1()</span><br></pre></td></tr></table></figure>



<p>tensor([[-0.5414, -1.0563,  0.2413,  0.1828,  0.6247, -0.7940, -0.6748, -0.3877],<br>     [ 0.6420, -0.8497, -0.6987, -0.2052, -0.7812,  0.6873,  0.7836, -1.1109],<br>     [-0.3359, -0.9029,  0.6440,  0.7592, -2.0203, -0.6740, -0.9192,  1.2120],<br>     [-1.2018, -0.5615, -0.9465, -0.7420,  0.1556, -0.2584, -0.7502,  1.2355]])<br> tensor([0, 0, 0, 0]) </p>
<h2 id="9-3-构建简单数据类型"><a href="#9-3-构建简单数据类型" class="headerlink" title="9.3 构建简单数据类型"></a>9.3 构建简单数据类型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test2</span>():</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 构建数据对象</span></span><br><span class="line">    x = torch.randn(<span class="number">100</span>,<span class="number">8</span>)</span><br><span class="line">    y = torch.randint(<span class="number">0</span>,<span class="number">2</span>,(x.size(<span class="number">0</span>),))</span><br><span class="line">    <span class="comment"># 使用TensorDataset也可以来构建简单的数据类，就不需要上面的class类了</span></span><br><span class="line">    sample_dataset = TensorDataset(x,y)</span><br><span class="line">    <span class="comment"># 使用Dataloader</span></span><br><span class="line">    dataloader = DataLoader(sample_dataset,</span><br><span class="line">                            batch_size=<span class="number">4</span>,</span><br><span class="line">                            shuffle=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> dataloader:</span><br><span class="line">        <span class="built_in">print</span>(x)</span><br><span class="line">        <span class="built_in">print</span>(y)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">test2()</span><br></pre></td></tr></table></figure>



<h2 id="9-4-使用组件创建线性回归"><a href="#9-4-使用组件创建线性回归" class="headerlink" title="9.4 使用组件创建线性回归"></a>9.4 使用组件创建线性回归</h2><h3 id="9-4-1-构建数据集"><a href="#9-4-1-构建数据集" class="headerlink" title="9.4.1 构建数据集"></a>9.4.1 构建数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_dataset</span>():</span><br><span class="line">    x, y, coef = make_regression(n_samples=<span class="number">100</span>,</span><br><span class="line">                                 n_features=<span class="number">1</span>,</span><br><span class="line">                                 noise=<span class="number">10</span>,</span><br><span class="line">                                 coef=<span class="literal">True</span>,</span><br><span class="line">                                 bias=<span class="number">14.5</span>,</span><br><span class="line">                                 random_state=<span class="number">42</span>)</span><br><span class="line">    <span class="comment"># 将构建的数据转换为张量类型</span></span><br><span class="line">    x = torch.tensor(x, dtype=torch.float32)</span><br><span class="line">    y = torch.tensor(y, dtype=torch.float32).view(-<span class="number">1</span>, <span class="number">1</span>)  <span class="comment"># 转换为列向量</span></span><br><span class="line">    <span class="keyword">return</span> x, y, coef</span><br></pre></td></tr></table></figure>



<h3 id="9-4-2-创建训练函数以及训练结果可视化"><a href="#9-4-2-创建训练函数以及训练结果可视化" class="headerlink" title="9.4.2 创建训练函数以及训练结果可视化"></a>9.4.2 创建训练函数以及训练结果可视化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>():</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建数据集</span></span><br><span class="line">    x, y, coef = create_dataset()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 构建数据对象</span></span><br><span class="line">    sample_dataset = TensorDataset(x, y)</span><br><span class="line">    <span class="comment"># 使用Dataloader</span></span><br><span class="line">    dataloader = DataLoader(sample_dataset,</span><br><span class="line">                            batch_size=<span class="number">16</span>,</span><br><span class="line">                            shuffle=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 构建线性模型</span></span><br><span class="line">    model = nn.Linear(in_features=<span class="number">1</span>, out_features=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 构建损失函数</span></span><br><span class="line">    criterion = nn.MSELoss()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 优化方法</span></span><br><span class="line">    optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化训练参数</span></span><br><span class="line">    epochs = <span class="number">1000</span></span><br><span class="line">    <span class="comment"># 记录每个epoch的损失</span></span><br><span class="line">    epoch_losses = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        epoch_loss = <span class="number">0.0</span></span><br><span class="line">        batch_count = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> train_x, train_y <span class="keyword">in</span> dataloader:</span><br><span class="line">            <span class="comment"># 将一个batch的数据输入模型</span></span><br><span class="line">            y_pred = model(train_x)</span><br><span class="line">            <span class="comment"># 计算损失</span></span><br><span class="line">            loss = criterion(y_pred, train_y.reshape(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">            <span class="comment"># 梯度清零</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            <span class="comment"># 反向传播</span></span><br><span class="line">            loss.backward()</span><br><span class="line">            <span class="comment"># 更新参数</span></span><br><span class="line">            optimizer.step()</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 累加损失</span></span><br><span class="line">            epoch_loss += loss.item()</span><br><span class="line">            batch_count += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算平均损失</span></span><br><span class="line">        avg_loss = epoch_loss / batch_count</span><br><span class="line">        epoch_losses.append(avg_loss)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 每10个epoch打印一次损失</span></span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>], Loss: <span class="subst">&#123;avg_loss:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ==== 绘制拟合曲线 ====</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">12</span>, <span class="number">5</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 子图1：原始数据和拟合线</span></span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 绘制原始数据点</span></span><br><span class="line">    plt.scatter(x.numpy(), y.numpy(), alpha=<span class="number">0.6</span>, label=<span class="string">&#x27;Original Data&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 生成测试数据用于绘制拟合线</span></span><br><span class="line">    x_test = torch.linspace(x.<span class="built_in">min</span>(), x.<span class="built_in">max</span>(), <span class="number">100</span>).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 禁用梯度计算</span></span><br><span class="line">        y_pred = model(x_test)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 绘制拟合线</span></span><br><span class="line">    plt.plot(x_test.numpy(), y_pred.numpy(), <span class="string">&#x27;r-&#x27;</span>, linewidth=<span class="number">2</span>, label=<span class="string">&#x27;Fitted Line&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获取模型参数</span></span><br><span class="line">    w, b = model.weight.item(), model.bias.item()</span><br><span class="line">    plt.title(<span class="string">f&#x27;Linear Regression: y = <span class="subst">&#123;w:<span class="number">.2</span>f&#125;</span>x + <span class="subst">&#123;b:<span class="number">.2</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">    plt.grid(<span class="literal">True</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ==== 绘制损失曲线 ====</span></span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">    plt.plot(<span class="built_in">range</span>(<span class="number">1</span>, epochs + <span class="number">1</span>), epoch_losses, <span class="string">&#x27;b-&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;Training Loss Curve&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">    plt.grid(<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 打印最终参数</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;\nTraining completed!&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Final parameters: weight = <span class="subst">&#123;w:<span class="number">.4</span>f&#125;</span>, bias = <span class="subst">&#123;b:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用训练函数</span></span><br><span class="line">train()</span><br></pre></td></tr></table></figure>



<img src="/2025/05/20/pytorch%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C/745a4667d1a84ed79926b44cbfd9c2c6.png" class="" title="img">
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">kukudelin</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2025/05/20/pytorch%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C/">http://example.com/2025/05/20/pytorch%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">小牛壮士</a>！</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="/img/%E6%96%87%E7%AB%A0%E9%BB%98%E8%AE%A4%E5%B0%81%E9%9D%A2.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/05/30/pytorch%E5%9F%BA%E6%9C%AC%E7%BB%84%E4%BB%B6%E7%9A%84%E4%BD%BF%E7%94%A8%E4%BB%A5%E5%8F%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%8C%E6%95%B4%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B%E4%BB%8B%E7%BB%8D/" title="pytorch基本组件的使用以及神经网络完整训练流程介绍"><img class="cover" src="/img/%E6%96%87%E7%AB%A0%E9%BB%98%E8%AE%A4%E5%B0%81%E9%9D%A2.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">pytorch基本组件的使用以及神经网络完整训练流程介绍</div></div><div class="info-2"><div class="info-item-1">前言：本篇文章是为后续更进一步学习经典神经网络，以及更先进的技术筑基，一定要牢牢掌握，年薪百万不是梦！！  一、Dataset1234from torch.utils.data import Datasetfrom PIL import Imageimport osimport random    用torch.utils.data中的Dataset组件来实现图片数据集的的合并以及标记标签的功能 1.1 MyData类这里需要我们自己来写一个MyData类用于初始化根目录，图片目录，并将他们拼接。以及实现取出单条数据和获取数据长度的方法 12345678910111213141516171819202122232425262728293031323334353637383940class MyData(Dataset):    def __init__(self,root_dir,label_dir):        &quot;&quot;&quot;        获取根目录和标签目录图片地址        将他们拼接后输出成图片相对路径地址列表        &quot;&...</div></div></div></a><a class="pagination-related" href="/2025/05/12/K-Means%E7%AE%97%E6%B3%95%E4%B8%8EDBSCAN%E7%AE%97%E6%B3%95%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E4%B8%8E%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/" title="K_Means算法与DBSCAN算法的工作流程与代码复现"><img class="cover" src="/img/%E6%96%87%E7%AB%A0%E9%BB%98%E8%AE%A4%E5%B0%81%E9%9D%A2.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">K_Means算法与DBSCAN算法的工作流程与代码复现</div></div><div class="info-2"><div class="info-item-1">前言​    前面的文章中，我们讲述了用于回归的线性回归算法，用于分类的逻辑回归算法以及其变式softmax函数，其实不管是回归还是分类，都是通过已有的数据集，知道输入和输出结果之间的关系，然后根据这种已知的关系，训练得到一个最优的模型。这样的模式被称为监督学习。但是在实际生产中，这样既有特征(feature)又有标签(label)的数据集似乎需要使用特殊设备或经过昂贵且用时非常长的实验过程进行人工标记才能得到。当我们不知道数据集中数据、特征之间的关系，而是要根据聚类或一定的模型才能得到数据之间的关系时，我们称为这样的模式叫做无监督学习。在无监督学习中数据只有特征(feature)无标签(label)，它本质上是一个统计手段，在没有标签的数据里可以发现潜在的一些结构的一种训练方式。 ​    本篇文章我们来讲讲无监督学习中，用于聚类的K_Means算法与DBSCAN算法。 一、聚类和分类​    聚类适用于没有明确类别标签的数据（无监督），目的是通过相似性将数据划分为若干个簇，以发现数据的内在结构和分布规律。它不需要预先定义类别，而是根据数据本身的特征自动分组。 ​    分类则...</div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/%E5%A4%B4%E5%83%8F.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">kukudelin</div><div class="author-info-description">林勇的个人博客</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">35</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/p98453"><i class="fab fa-github"></i><span>点这里！查看项目🎯</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://blog.csdn.net/hdsbdjsjsbs?type=blog" target="_blank" title="CSDN"><i class="fa-solid fa-blog fa-bounce" style="color: #fc5531;"></i></a><a class="social-icon" href="mailto:3224688576@qq.com" target="_blank" title="Email"><i class="fa-brands fa-qq fa-bounce" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到我的小站</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%89%8D%E8%A8%80%EF%BC%9A"><span class="toc-number">1.</span> <span class="toc-text">前言：</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E5%88%9B%E5%BB%BA%E5%BC%A0%E9%87%8F"><span class="toc-number">2.</span> <span class="toc-text">一、创建张量</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-%E7%AE%80%E5%8D%95%E5%BC%A0%E9%87%8F"><span class="toc-number">2.1.</span> <span class="toc-text">1.1 简单张量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-%E7%BA%BF%E6%80%A7%E5%BC%A0%E9%87%8F%E5%92%8C%E9%9A%8F%E6%9C%BA%E5%BC%A0%E9%87%8F"><span class="toc-number">2.2.</span> <span class="toc-text">1.2 线性张量和随机张量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3-%E6%8C%87%E5%AE%9A%E5%80%BC%E5%BC%A0%E9%87%8F"><span class="toc-number">2.3.</span> <span class="toc-text">1.3 指定值张量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-4-%E5%BC%A0%E9%87%8F%E5%85%83%E7%B4%A0%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2"><span class="toc-number">2.4.</span> <span class="toc-text">1.4 张量元素类型转换</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97"><span class="toc-number">3.</span> <span class="toc-text">二、数值计算</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-%E5%BC%A0%E9%87%8F%E7%9A%84%E5%9F%BA%E6%9C%AC%E8%BF%90%E7%AE%97"><span class="toc-number">3.1.</span> <span class="toc-text">2.1 张量的基本运算</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-%E9%98%BF%E8%BE%BE%E7%8E%9B%E7%A7%AF"><span class="toc-number">3.2.</span> <span class="toc-text">2.2 阿达玛积</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-%E7%82%B9%E7%A7%AF%E8%BF%90%E7%AE%97"><span class="toc-number">3.3.</span> <span class="toc-text">2.3 点积运算</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89%E3%80%81-%E6%95%B0%E5%80%BC%E8%BD%AC%E6%8D%A2"><span class="toc-number">4.</span> <span class="toc-text">三、 数值转换</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-tensor%E5%BC%A0%E9%87%8F%E8%BD%ACnumpy%E6%95%B0%E7%BB%84"><span class="toc-number">4.1.</span> <span class="toc-text">3.1 tensor张量转numpy数组</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-numpy%E6%95%B0%E7%BB%84%E8%BD%ACtensor%E5%BC%A0%E9%87%8F"><span class="toc-number">4.2.</span> <span class="toc-text">3.2 numpy数组转tensor张量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-%E6%A0%87%E9%87%8F%E5%BC%A0%E9%87%8F%E5%92%8C%E6%95%B0%E5%AD%97%E7%9A%84%E8%BD%AC%E6%8D%A2"><span class="toc-number">4.3.</span> <span class="toc-text">3.3 标量张量和数字的转换</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E6%8B%BC%E6%8E%A5%E6%93%8D%E4%BD%9C"><span class="toc-number">5.</span> <span class="toc-text">四、拼接操作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-torch-cat%EF%BC%88%E8%BF%9E%E6%8E%A5%E6%93%8D%E4%BD%9C%EF%BC%89"><span class="toc-number">5.1.</span> <span class="toc-text">4.1 torch.cat（连接操作）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-torch-stack%EF%BC%88%E5%A0%86%E5%8F%A0%E6%93%8D%E4%BD%9C%EF%BC%89"><span class="toc-number">5.2.</span> <span class="toc-text">4.2 torch.stack（堆叠操作）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E7%B4%A2%E5%BC%95%E6%93%8D%E4%BD%9C"><span class="toc-number">6.</span> <span class="toc-text">五、索引操作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1-%E7%AE%80%E5%8D%95%E8%A1%8C%E5%88%97%E7%B4%A2%E5%BC%95"><span class="toc-number">6.1.</span> <span class="toc-text">5.1 简单行列索引</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2-%E5%B8%83%E5%B0%94%E7%B4%A2%E5%BC%95"><span class="toc-number">6.2.</span> <span class="toc-text">5.2 布尔索引</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-3-%E5%A4%9A%E7%BB%B4%E7%B4%A2%E5%BC%95"><span class="toc-number">6.3.</span> <span class="toc-text">5.3 多维索引</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%AD%E3%80%81%E5%BD%A2%E7%8A%B6%E6%93%8D%E4%BD%9C"><span class="toc-number">7.</span> <span class="toc-text">六、形状操作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-1-reshape-%E5%87%BD%E6%95%B0"><span class="toc-number">7.1.</span> <span class="toc-text">6.1 reshape()函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-2-transpose-%E5%87%BD%E6%95%B0%E5%92%8Cpermute-%E5%87%BD%E6%95%B0"><span class="toc-number">7.2.</span> <span class="toc-text">6.2 transpose()函数和permute()函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-3-view-%E5%87%BD%E6%95%B0%E5%92%8Ccontigous-%E5%87%BD%E6%95%B0"><span class="toc-number">7.3.</span> <span class="toc-text">6.3 view()函数和contigous()函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-4-squeeze-%E5%87%BD%E6%95%B0%E5%92%8Cunsqueeze-%E5%87%BD%E6%95%B0"><span class="toc-number">7.4.</span> <span class="toc-text">6.4 squeeze()函数和unsqueeze()函数</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%83%E3%80%81%E8%BF%90%E7%AE%97%E5%87%BD%E6%95%B0"><span class="toc-number">8.</span> <span class="toc-text">七、运算函数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%AB%E3%80%81%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">9.</span> <span class="toc-text">八、反向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#8-1-%E6%A2%AF%E5%BA%A6%E5%9F%BA%E6%9C%AC%E8%AE%A1%E7%AE%97"><span class="toc-number">9.1.</span> <span class="toc-text">8.1 梯度基本计算</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-1-1-%E6%A0%87%E9%87%8F%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-number">9.1.1.</span> <span class="toc-text">8.1.1 标量梯度计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-1-2-%E5%90%91%E9%87%8F%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-number">9.1.2.</span> <span class="toc-text">8.1.2 向量梯度计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-1-3-%E5%A4%9A%E6%A0%87%E9%87%8F%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-number">9.1.3.</span> <span class="toc-text">8.1.3 多标量梯度计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-1-4-%E5%A4%9A%E5%90%91%E9%87%8F%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-number">9.1.4.</span> <span class="toc-text">8.1.4 多向量梯度计算</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-2-%E6%8E%A7%E5%88%B6%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-number">9.2.</span> <span class="toc-text">8.2 控制梯度计算</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-2-2-%E7%B4%AF%E8%AE%A1%E6%A2%AF%E5%BA%A6%E5%92%8C%E6%A2%AF%E5%BA%A6%E6%B8%85%E9%9B%B6"><span class="toc-number">9.2.1.</span> <span class="toc-text">8.2.2 累计梯度和梯度清零</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-2-3-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E6%A1%88%E4%BE%8B"><span class="toc-number">9.2.2.</span> <span class="toc-text">8.2.3 梯度下降优化算法案例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-3-%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97%E6%B3%A8%E6%84%8F"><span class="toc-number">9.3.</span> <span class="toc-text">8.3 梯度计算注意</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-3-1-%E7%AE%80%E5%8D%95%E6%BC%94%E7%A4%BA"><span class="toc-number">9.3.1.</span> <span class="toc-text">8.3.1 简单演示</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-3-2-%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%EF%BC%9Adetach-%E6%95%B0%E6%8D%AE%E5%85%B1%E4%BA%AB"><span class="toc-number">9.3.2.</span> <span class="toc-text">8.3.2 解决方案：detach()数据共享</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B9%9D%E3%80%81%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6"><span class="toc-number">10.</span> <span class="toc-text">九、基础组件</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#9-1-%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6%E7%9A%84%E7%94%A8%E6%B3%95"><span class="toc-number">10.1.</span> <span class="toc-text">9.1 基础组件的用法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#9-1-1-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">10.1.1.</span> <span class="toc-text">9.1.1 损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-1-2-%E7%BA%BF%E6%80%A7%E5%81%87%E8%AE%BE%E5%87%BD%E6%95%B0"><span class="toc-number">10.1.2.</span> <span class="toc-text">9.1.2 线性假设函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-1-3-%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">10.1.3.</span> <span class="toc-text">9.1.3 优化器</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-2-%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E5%99%A8%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">10.2.</span> <span class="toc-text">9.2 数据加载器的使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#9-2-1-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%88%9B%E5%BB%BA"><span class="toc-number">10.2.1.</span> <span class="toc-text">9.2.1 数据类创建</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-2-2-%E5%AE%9E%E4%BE%8B%E5%8C%96%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E7%B1%BB"><span class="toc-number">10.2.2.</span> <span class="toc-text">9.2.2 实例化数据加载类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-2-3-%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E7%B1%BB%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">10.2.3.</span> <span class="toc-text">9.2.3 数据加载类的使用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-3-%E6%9E%84%E5%BB%BA%E7%AE%80%E5%8D%95%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="toc-number">10.3.</span> <span class="toc-text">9.3 构建简单数据类型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-4-%E4%BD%BF%E7%94%A8%E7%BB%84%E4%BB%B6%E5%88%9B%E5%BB%BA%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">10.4.</span> <span class="toc-text">9.4 使用组件创建线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#9-4-1-%E6%9E%84%E5%BB%BA%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">10.4.1.</span> <span class="toc-text">9.4.1 构建数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-4-2-%E5%88%9B%E5%BB%BA%E8%AE%AD%E7%BB%83%E5%87%BD%E6%95%B0%E4%BB%A5%E5%8F%8A%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">10.4.2.</span> <span class="toc-text">9.4.2 创建训练函数以及训练结果可视化</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/08/20/%E5%9F%BA%E4%BA%8EAutoDL%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84VLLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E4%BB%A5%E5%8F%8A%E6%9C%AC%E5%9C%B0%E8%B0%83%E7%94%A8/" title="基于AutoDL云服务器的VLLM大模型部署以及本地调用"><img src="/img/%E6%96%87%E7%AB%A0%E9%BB%98%E8%AE%A4%E5%B0%81%E9%9D%A2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="基于AutoDL云服务器的VLLM大模型部署以及本地调用"/></a><div class="content"><a class="title" href="/2025/08/20/%E5%9F%BA%E4%BA%8EAutoDL%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84VLLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E4%BB%A5%E5%8F%8A%E6%9C%AC%E5%9C%B0%E8%B0%83%E7%94%A8/" title="基于AutoDL云服务器的VLLM大模型部署以及本地调用">基于AutoDL云服务器的VLLM大模型部署以及本地调用</a><time datetime="2025-08-20T10:34:09.000Z" title="发表于 2025-08-20 18:34:09">2025-08-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/08/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98/" title="机器学习面试题"><img src="/img/%E6%96%87%E7%AB%A0%E9%BB%98%E8%AE%A4%E5%B0%81%E9%9D%A2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="机器学习面试题"/></a><div class="content"><a class="title" href="/2025/08/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98/" title="机器学习面试题">机器学习面试题</a><time datetime="2025-08-20T08:03:59.000Z" title="发表于 2025-08-20 16:03:59">2025-08-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/08/15/Tokenizer%EF%BC%88%E5%88%87%E8%AF%8D%E5%99%A8%EF%BC%89%E7%9A%84%E4%B8%8D%E5%90%8C%E5%AE%9E%E7%8E%B0%E7%AE%97%E6%B3%95/" title="Tokenizer（切词器）的不同实现算法"><img src="/img/%E6%96%87%E7%AB%A0%E9%BB%98%E8%AE%A4%E5%B0%81%E9%9D%A2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Tokenizer（切词器）的不同实现算法"/></a><div class="content"><a class="title" href="/2025/08/15/Tokenizer%EF%BC%88%E5%88%87%E8%AF%8D%E5%99%A8%EF%BC%89%E7%9A%84%E4%B8%8D%E5%90%8C%E5%AE%9E%E7%8E%B0%E7%AE%97%E6%B3%95/" title="Tokenizer（切词器）的不同实现算法">Tokenizer（切词器）的不同实现算法</a><time datetime="2025-08-15T10:43:20.000Z" title="发表于 2025-08-15 18:43:20">2025-08-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/08/14/Transformer%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E8%A7%A3%E6%9E%90/" title="Transformer网络结构解析"><img src="/img/%E6%96%87%E7%AB%A0%E9%BB%98%E8%AE%A4%E5%B0%81%E9%9D%A2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer网络结构解析"/></a><div class="content"><a class="title" href="/2025/08/14/Transformer%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E8%A7%A3%E6%9E%90/" title="Transformer网络结构解析">Transformer网络结构解析</a><time datetime="2025-08-14T02:24:51.000Z" title="发表于 2025-08-14 10:24:51">2025-08-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/08/13/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" title="Transformer自注意力机制"><img src="/img/%E6%96%87%E7%AB%A0%E9%BB%98%E8%AE%A4%E5%B0%81%E9%9D%A2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer自注意力机制"/></a><div class="content"><a class="title" href="/2025/08/13/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" title="Transformer自注意力机制">Transformer自注意力机制</a><time datetime="2025-08-13T08:40:23.000Z" title="发表于 2025-08-13 16:40:23">2025-08-13</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 By kukudelin</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.4.3</a></span></div><div class="footer_custom_text">平静的大海培养不出优秀的水手</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div></div></body></html>